{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'en_to_vi.txt'\n",
    "dataset = tf.data.TextLineDataset(file_name)\n",
    "\n",
    "def do_map(ds):\n",
    "  splited = tf.strings.split(ds,sep='\\t')\n",
    "  inp = tf.squeeze(tf.slice(splited, [0], [1]))\n",
    "  targ = tf.squeeze(tf.slice(splited, [1], [1]))\n",
    "  return (inp, targ)\n",
    "\n",
    "examples = dataset.map(lambda ds:do_map(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = examples\n",
    "train_en = train_examples.map(lambda pt, en: en)\n",
    "train_pt = train_examples.map(lambda pt, en: pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'C\\xc3\\xa2u chuy\\xe1\\xbb\\x87n b\\xe1\\xba\\xaft \\xc4\\x91\\xe1\\xba\\xa7u v\\xe1\\xbb\\x9bi bu\\xe1\\xbb\\x95i l\\xe1\\xbb\\x85 \\xc4\\x91\\xe1\\xba\\xbfm ng\\xc6\\xb0\\xe1\\xbb\\xa3c.'\n",
      "b'Ng\\xc3\\xa0y 14, th\\xc3\\xa1ng 8, n\\xc4\\x83m 1947, g\\xe1\\xba\\xa7n n\\xe1\\xbb\\xada \\xc4\\x91\\xc3\\xaam, \\xe1\\xbb\\x9f Bombay, c\\xc3\\xb3 m\\xe1\\xbb\\x99t ph\\xe1\\xbb\\xa5 n\\xe1\\xbb\\xaf s\\xe1\\xba\\xafp l\\xc3\\xa2m b\\xe1\\xbb\\x93n.'\n",
      "b'C\\xc3\\xb9ng l\\xc3\\xbac, tr\\xc3\\xaan kh\\xe1\\xba\\xafp \\xc4\\x91\\xe1\\xba\\xa5t \\xe1\\xba\\xa4n, ng\\xc6\\xb0\\xe1\\xbb\\x9di ta n\\xc3\\xadn th\\xe1\\xbb\\x9f ch\\xe1\\xbb\\x9d \\xc4\\x91\\xe1\\xbb\\xa3i tuy\\xc3\\xaan ng\\xc3\\xb4n \\xc4\\x91\\xe1\\xbb\\x99c l\\xe1\\xba\\xadp sau g\\xe1\\xba\\xa7n hai th\\xe1\\xba\\xadp k\\xe1\\xbb\\xb7 l\\xc3\\xa0 thu\\xe1\\xbb\\x99c \\xc4\\x91\\xe1\\xbb\\x8ba c\\xe1\\xbb\\xa7a Anh.'\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer_params = dict(lower_case=True)\n",
    "en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)\n",
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "    for ex in en_examples:\n",
    "        print(ex.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the examples -> (batch, word, word-piece)\n",
    "token_batch = en_tokenizer.tokenize(en_examples)\n",
    "# Merge the word and word-piece axes -> (batch, tokens)\n",
    "token_batch = token_batch.merge_dims(-2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([b'C\\xc3\\xa2u', b'chuy\\xe1\\xbb\\x87n', b'b\\xe1\\xba\\xaft',\n",
       "              b'\\xc4\\x91\\xe1\\xba\\xa7u', b'v\\xe1\\xbb\\x9bi', b'bu\\xe1\\xbb\\x95i',\n",
       "              b'l\\xe1\\xbb\\x85', b'\\xc4\\x91\\xe1\\xba\\xbfm',\n",
       "              b'ng\\xc6\\xb0\\xe1\\xbb\\xa3c', b'.'], dtype=object)                ,\n",
       "       array([b'Ng\\xc3\\xa0y', b'14', b',', b'th\\xc3\\xa1ng', b'8', b',',\n",
       "              b'n\\xc4\\x83m', b'1947', b',', b'g\\xe1\\xba\\xa7n', b'n\\xe1\\xbb\\xada',\n",
       "              b'\\xc4\\x91\\xc3\\xaam', b',', b'\\xe1\\xbb\\x9f', b'Bombay', b',',\n",
       "              b'c\\xc3\\xb3', b'm\\xe1\\xbb\\x99t', b'ph\\xe1\\xbb\\xa5',\n",
       "              b'n\\xe1\\xbb\\xaf', b's\\xe1\\xba\\xafp', b'l\\xc3\\xa2m',\n",
       "              b'b\\xe1\\xbb\\x93n', b'.'], dtype=object)                            ,\n",
       "       array([b'C\\xc3\\xb9ng', b'l\\xc3\\xbac', b',', b'tr\\xc3\\xaan',\n",
       "              b'kh\\xe1\\xba\\xafp', b'\\xc4\\x91\\xe1\\xba\\xa5t', b'\\xe1\\xba\\xa4n',\n",
       "              b',', b'ng\\xc6\\xb0\\xe1\\xbb\\x9di', b'ta', b'n\\xc3\\xadn',\n",
       "              b'th\\xe1\\xbb\\x9f', b'ch\\xe1\\xbb\\x9d', b'\\xc4\\x91\\xe1\\xbb\\xa3i',\n",
       "              b'tuy\\xc3\\xaan', b'ng\\xc3\\xb4n', b'\\xc4\\x91\\xe1\\xbb\\x99c',\n",
       "              b'l\\xe1\\xba\\xadp', b'sau', b'g\\xe1\\xba\\xa7n', b'hai',\n",
       "              b'th\\xe1\\xba\\xadp', b'k\\xe1\\xbb\\xb7', b'l\\xc3\\xa0',\n",
       "              b'thu\\xe1\\xbb\\x99c', b'\\xc4\\x91\\xe1\\xbb\\x8ba', b'c\\xe1\\xbb\\xa7a',\n",
       "              b'Anh', b'.'], dtype=object)                                     ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = en_tokenizer.detokenize(token_batch).numpy()\n",
    "arr"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e54e8dae9f6fcf4339e0c8fda7f5d6588fbdd9fd2066dc042bfc22c43eb1f5c1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
